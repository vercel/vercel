name: E2E Tests
# any files changing in .github/workflows will ensure
# that all e2e tests run. One easy way of doing this
# is to increment the counter below.
# cache buster: 2

on:
  pull_request:

env:
  VERCEL_TELEMETRY_DISABLED: '1'
  TURBO_REMOTE_ONLY: 'true'
  TURBO_TEAM: 'vercel'
  TURBO_TOKEN: ${{ secrets.TURBO_TOKEN }}

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: ${{ github.ref != 'refs/heads/main' }}

jobs:
  setup:
    name: Find Changes
    runs-on: ubuntu-latest
    outputs:
      tests: ${{ steps['set-tests'].outputs['tests'] }}
      dplUrl: ${{ steps.resolveTarball.outputs.url }}
      affectedPackages: ${{ steps['affected-packages'].outputs['packages'] }}
      testStrategy: ${{ steps['affected-packages'].outputs['strategy'] }}
      affectedCount: ${{ steps['affected-packages'].outputs['count'] }}
      totalCount: ${{ steps['affected-packages'].outputs['total'] }}
      allPackages: ${{ steps['affected-packages'].outputs['allPackages'] }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
          token: ${{ secrets.GH_TOKEN_PULL_REQUESTS }}
      - uses: actions/setup-node@v4
        with:
          node-version: 22
      - name: Setup Rust toolchain
        uses: dtolnay/rust-toolchain@f7ccc83f9ed1e5b9c81d8a67d7ad1a747e22a561 # 2025-12-16
        with:
          toolchain: stable
          targets: wasm32-wasip2
      - name: install pnpm@8.3.1
        run: npm i -g pnpm@8.3.1
      - run: pnpm install
      - id: affected-packages
        run: |
          export TURBO_BASE_SHA="${{ github.event.pull_request.base.sha }}"

          AFFECTED_OUTPUT=$(node utils/test-affected.js "${{ github.event.pull_request.base.sha }}" 2>&1)

          PACKAGE_COUNT=$(echo "$AFFECTED_OUTPUT" | grep -o "Found [0-9]* affected packages" | grep -o "[0-9]*" | head -1 || echo "0")
          TOTAL_COUNT=$(echo "$AFFECTED_OUTPUT" | grep -o "Total packages with tests: [0-9]*" | grep -o "[0-9]*" | head -1 || echo "0")

          if echo "$AFFECTED_OUTPUT" | grep -q "config or workflow changes detected"; then
            STRATEGY="all-e2e"
          elif echo "$AFFECTED_OUTPUT" | grep -q "test-all result detected"; then
            STRATEGY="test-all"
          elif [ "$PACKAGE_COUNT" -gt "0" ]; then
            STRATEGY="affected-only"
          else
            STRATEGY="no-tests"
          fi

          PACKAGES=$(echo "$AFFECTED_OUTPUT" | sed -n '/Affected packages that would be tested:/,/This would result in the following turbo filters:/p' | grep "  - " | sed 's/  - //' | tr '\n' ',' | sed 's/,$//')

          ALL_PACKAGES=$(echo "$AFFECTED_OUTPUT" | grep "All packages with tests:" | sed 's/All packages with tests: //' | head -1 | xargs)

          echo "PACKAGE_COUNT=$PACKAGE_COUNT"
          echo "TOTAL_COUNT=$TOTAL_COUNT"
          echo "STRATEGY=$STRATEGY"

          echo "strategy=$STRATEGY" >> $GITHUB_OUTPUT
          echo "packages=$PACKAGES" >> $GITHUB_OUTPUT
          echo "count=$PACKAGE_COUNT" >> $GITHUB_OUTPUT
          echo "total=$TOTAL_COUNT" >> $GITHUB_OUTPUT
          echo "allPackages=$ALL_PACKAGES" >> $GITHUB_OUTPUT
      - id: set-tests
        run: |
          TESTS_ARRAY=$(node utils/chunk-tests.js)
          echo "E2E tests to run:"
          echo "$TESTS_ARRAY"
          echo "tests=$TESTS_ARRAY" >> $GITHUB_OUTPUT
        env:
          TURBO_BASE_SHA: ${{ github.event.pull_request.base.sha }}
          TEST_TYPE: e2e
      - uses: patrickedqvist/wait-for-vercel-preview@bfdff514ff78a669f2536e9f4dd4ef5813a704a2
        id: waitForTarball
        continue-on-error: true
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          max_timeout: 360
          check_interval: 5
      - name: Resolve Vercel tarball URL
        id: resolveTarball
        uses: actions/github-script@v7
        env:
          WAIT_URL: ${{ steps.waitForTarball.outputs.url }}
        with:
          script: |
            const owner = context.repo.owner;
            const repo = context.repo.repo;
            const prNumber =
              context.payload.pull_request?.number ?? context.issue.number;

            const waitUrl = process.env.WAIT_URL || '';
            if (waitUrl) {
              core.info(`Using deployment URL from wait step: ${waitUrl}`);
              core.setOutput('url', waitUrl);
              return;
            }

            core.info(
              'No deployment found for the current HEAD commit. Falling back to the most recent successful Vercel preview deployment for this PR.'
            );

            const { data: commits } = await github.rest.pulls.listCommits({
              owner,
              repo,
              pull_number: prNumber,
              per_page: 100,
            });

            const shas = commits.map(c => c.sha).reverse();

            for (const sha of shas) {
              core.info(`Checking deployments for ${sha}...`);

              const { data: deployments } =
                await github.rest.repos.listDeployments({
                  owner,
                  repo,
                  sha,
                  environment: 'Preview',
                  per_page: 5,
                });

              for (const deployment of deployments) {
                if (deployment.creator?.login !== 'vercel[bot]') {
                  continue;
                }

                const { data: statuses } =
                  await github.rest.repos.listDeploymentStatuses({
                    owner,
                    repo,
                    deployment_id: deployment.id,
                    per_page: 10,
                  });

                const success = statuses.find(
                  s => s.state === 'success' && s.environment_url
                );

                if (success?.environment_url) {
                  core.info(`Resolved deployment URL: ${success.environment_url}`);
                  core.setOutput('url', success.environment_url);
                  return;
                }
              }
            }

            core.setFailed(
              'No Vercel preview deployment found for any commit in this PR.'
            );

  test:
    timeout-minutes: 120
    runs-on: ${{ matrix.runner }}
    name: ${{matrix.scriptName}} (${{matrix.packageName}}, ${{matrix.chunkNumber}}, ${{ matrix.runner }}, Node v${{ matrix.nodeVersion }})
    if: ${{ needs.setup.outputs['tests'] != '[]' }}
    needs:
      - setup
    strategy:
      fail-fast: false
      matrix:
        include: ${{ fromJson(needs.setup.outputs['tests']) }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.nodeVersion }}

      - name: install yarn@1.22.19
        run: npm i -g yarn@1.22.19

      - name: install pnpm@8.3.1
        run: npm i -g pnpm@8.3.1

      - run: pnpm install

      - name: Build ${{matrix.packageName}} and all its dependencies
        run: node utils/gen.js && node_modules/.bin/turbo run build --cache-dir=".turbo" --log-order=stream --filter=${{matrix.packageName}}...
        env:
          FORCE_COLOR: '1'
      - name: Resolve Python runtime wheel URL
        id: python-wheel
        run: |
          WHEEL_NAME=$(curl -sf "${{ needs.setup.outputs.dplUrl }}/tarballs/vercel-runtime-wheel.json" | jq -r '.filename' || echo "")
          if [ -n "$WHEEL_NAME" ]; then
            echo "spec=vercel-runtime @ ${{ needs.setup.outputs.dplUrl }}/tarballs/$WHEEL_NAME" >> $GITHUB_OUTPUT
          else
            echo "spec=" >> $GITHUB_OUTPUT
          fi
      - name: Test ${{matrix.packageName}}
        run: node utils/gen.js && node_modules/.bin/turbo run ${{matrix.testScript}} --summarize --cache-dir=".turbo" --log-order=stream --filter=${{matrix.packageName}} -- ${{ join(matrix.testPaths, ' ') }}
        shell: bash
        env:
          JEST_JUNIT_OUTPUT_FILE: ${{github.workspace}}/.junit-reports/${{matrix.scriptName}}-${{matrix.packageName}}-${{matrix.chunkNumber}}-${{ matrix.runner }}.xml
          VERCEL_CLI_VERSION: ${{ needs.setup.outputs.dplUrl }}/tarballs/vercel.tgz
          VERCEL_RUNTIME_PYTHON: ${{ steps.python-wheel.outputs.spec }}
          VERCEL_TOKEN: ${{ secrets.VERCEL_TOKEN }}
          VERCEL_TEAM_ID: ${{ secrets.VERCEL_TEAM_ID }}
          TURBO_BASE_SHA: ${{ github.event.pull_request.base.sha }}
          FORCE_COLOR: '1'
      - name: 'Determine Turbo HIT or MISS'
        if: ${{ !cancelled() }}
        id: turbo-summary
        shell: bash
        run: |
          TURBO_MISS_COUNT=`node utils/determine-turbo-hit-or-miss.js`
          echo "MISS COUNT: $TURBO_MISS_COUNT"
          echo "misses=$TURBO_MISS_COUNT" >> $GITHUB_OUTPUT
      - name: 'Upload Test Report to Datadog'
        if: ${{ steps['turbo-summary'].outputs.misses != '0' && !cancelled() }}
        run: 'pnpm dlx @datadog/datadog-ci@4.2.2 junit upload --service vercel-cli .junit-reports'
        env:
          DATADOG_API_KEY: ${{secrets.DATADOG_API_KEY_CLI}}
          DD_ENV: ci

  summary:
    name: Summary (e2e)
    runs-on: ubuntu-latest
    timeout-minutes: 5
    if: always()
    needs:
      - test
    steps:
      - name: Check All
        run: |-
          for status in ${{ join(needs.*.result, ' ') }}
          do
            if [ "$status" != "success" ] && [ "$status" != "skipped" ]
            then
              echo "Some checks failed"
              exit 1
            fi
          done
